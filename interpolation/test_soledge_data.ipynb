{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8628e897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post1-py3-none-any.whl\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2977624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/78k/Desktop/test_soledge2gitr/train.py\", line 2, in <module>\r\n",
      "    import nn_learner\r\n",
      "  File \"/Users/78k/Desktop/test_soledge2gitr/nn_learner.py\", line 11, in <module>\r\n",
      "    import sklearn.metrics\r\n",
      "ModuleNotFoundError: No module named 'sklearn'\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be5f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad72f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022, SOLEDGE contributors\n",
    "# Authors: Nick Lubbers(LANL), Abdou Diaw (ORNL)\n",
    "# License: 3-Clause-BSD-LBNL\n",
    "\"\"\"\n",
    "This file is part SOLEDGE-GITR.\n",
    "It uses an ensemble of NN to train and test dataset from DB.\n",
    "\"\"\"\n",
    "# -------\n",
    "# Imports\n",
    "# -------\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# Built with numpy '1.16.3'\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "# Built with pytorch 1.3.1\n",
    "\n",
    "import sklearn.metrics\n",
    "# Built with sklearn 0.20.1\n",
    "\n",
    "import sqlite3\n",
    "from enum import Enum, IntEnum\n",
    "import collections\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Define inputs/ouputs of the problem\n",
    "\n",
    "Inputs = collections.namedtuple('Inputs', 'R Z')\n",
    "Outputs = collections.namedtuple('Outputs', 'BPHI BR BZ VE NE TE VI NI TI')\n",
    "\n",
    "\n",
    "def getAllGNDData(dbPath):\n",
    "    selString = \"\"\n",
    "\n",
    "    selString = \"SELECT * FROM SOLEDGE;\"\n",
    "    sqlDB = sqlite3.connect(dbPath, timeout=45.0)\n",
    "    sqlCursor = sqlDB.cursor()\n",
    "    gndResults = []\n",
    "    for row in sqlCursor.execute(selString):\n",
    "        # Add row to numpy array\n",
    "        gndResults.append(row)\n",
    "    sqlCursor.close()\n",
    "    sqlDB.close()\n",
    "    return np.array(gndResults)\n",
    "\n",
    "class Model():\n",
    "    \"\"\"\n",
    "    Ensemble model. Error bar is std. dev. of networks.\n",
    "    \"\"\"\n",
    "    def __init__(self,networks,err_info=None):\n",
    "        self.networks = networks\n",
    "        self.err_info = err_info\n",
    "\n",
    "    def __call__(self,request_params):\n",
    "        \"\"\"\n",
    "        :param request_params: Inputs\n",
    "        :return: result[3], errbar[3]\n",
    "        \"\"\"\n",
    "        request_params = self.pack_inputs(request_params)\n",
    "\n",
    "        result_mean,result_error = self.process(request_params)\n",
    "\n",
    "        result_mean = self.unpack_outputs(result_mean)\n",
    "        result_error = self.unpack_outputs(result_error)\n",
    "        return result_mean,result_error\n",
    "\n",
    "\n",
    "    def process(self,request_params,perform_column_extraction=True): #as a numpy array\n",
    "        batched = request_params.ndim>1\n",
    "\n",
    "        request_params = torch.as_tensor(request_params)\n",
    "        if perform_column_extraction:\n",
    "            request_params = request_params[...,SOLVER_INDEXES[\"input_slice\"]]\n",
    "\n",
    "        if not batched:\n",
    "            request_params = request_params.unsqueeze(0)\n",
    "\n",
    "        results = np.asarray([np.asarray(nn(request_params)) for nn in self.networks])\n",
    "\n",
    "        mean = results.mean(axis=0)\n",
    "        std = results.std(axis=0)\n",
    "\n",
    "        if not batched:\n",
    "            mean = mean[0]\n",
    "            std = std[0]\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "    def calibrate(self,dataset):\n",
    "        pred,uncertainty = self.process(dataset[:][0].numpy(),perform_column_extraction=False)\n",
    "        true = dataset[:][-1].numpy()\n",
    "        abserr = np.abs(pred-true)\n",
    "\n",
    "        calibration = np.mean(abserr,axis=0)/np.mean(uncertainty,axis=0)\n",
    "\n",
    "        calibration /= 3\n",
    "        self.err_info /= calibration\n",
    "\n",
    "        inactive_columns = (np.std(abserr,axis=0)==0) & (np.std(uncertainty,axis=0)==0)\n",
    "\n",
    "        warnings.warn(\"Inactive columns detected: {} , they will not be included in isokay.\".format(np.where(inactive_columns)))\n",
    "        self.err_info[inactive_columns]=np.inf\n",
    "\n",
    "\n",
    "    def process_iserrok_fuzzy(self,errbars):\n",
    "        return errbars/self.err_info\n",
    "\n",
    "    def process_iserrok(self,errbars):\n",
    "        return errbars < self.err_info\n",
    "\n",
    "    def iserrok(self,errbars):\n",
    "        errbars = self.pack_outputs(errbars)\n",
    "        iserrok = self.process_iserrok(errbars)\n",
    "        iserrok = self.unpack_outputs(iserrok)\n",
    "        return iserrok\n",
    "\n",
    "    def iserrok_fuzzy(self,errbars):\n",
    "        errbars = self.pack_outputs(errbars)\n",
    "        iserrok = self.process_iserrok_fuzzy(errbars)\n",
    "        iserrok = self.unpack_outputs(iserrok)\n",
    "        return iserrok\n",
    "\n",
    "    def pack_outputs(self,request):\n",
    "        return NotImplemented\n",
    "\n",
    "    def pack_inputs(self,request):\n",
    "        return NotImplemented\n",
    "\n",
    "    def unpack_outputs(self,result):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class LearnerModel(Model):\n",
    "    def pack_inputs(self,request):\n",
    "        packed_request = np.concatenate([[request.R],[request.Z]])\n",
    "        return packed_request\n",
    "\n",
    "    def unpack_outputs(self,packed_result):\n",
    "        unpacked_result = Outputs(packed_result[0],packed_result[1],\n",
    "                                  packed_result[2],packed_result[3],\n",
    "                                  packed_result[4],packed_result[5],\n",
    "                                  packed_result[6],packed_result[7],\n",
    "                                  packed_result[8]\n",
    "                                 )\n",
    "        return unpacked_result\n",
    "\n",
    "    def pack_outputs(self,outputs):\n",
    "        output_vals = np.concatenate([[ouputs.BPHI], [outputs.BR], [outputs.BZ], [outputs.VE], [outputs.NE], [outputs.TE],\n",
    "         [outputs.VI], [outputs.NI], [outputs.TI]])\n",
    "        return output_vals,\n",
    "\n",
    "# Parameters governing input and outputs to problem\n",
    "SOLVER_INDEXES = dict(\n",
    "        input_slice = slice(0,2),\n",
    "        output_slice = slice(2,11),\n",
    "        n_inputs = 2,\n",
    "        n_outputs = 9,\n",
    ")\n",
    "\n",
    "#Parameters governing network structure\n",
    "DEFAULT_NET_CONFIG = dict(\n",
    "    n_layers=6,\n",
    "    n_hidden=64,\n",
    "    activation_type=torch.nn.ReLU,\n",
    "    layer_type = torch.nn.Linear\n",
    "    )\n",
    "\n",
    "#Parameters for ensemble uncertainty\n",
    "DEFAULT_ENSEMBLE_CONFIG = dict(\n",
    "    n_members = 2, #5,\n",
    "    test_fraction = 0.1, #0.5,\n",
    "    score_thresh = 0.2, #0.7,\n",
    "    max_model_tries = 200,\n",
    ")\n",
    "\n",
    "\n",
    "#Parameters for the optimization process\n",
    "DEFAULT_TRAINING_CONFIG = dict(\n",
    "    n_epochs=200, #00,\n",
    "    optimizer_type=torch.optim.Adam,\n",
    "    validation_fraction=0.1, #2,\n",
    "    lr=1e-3,\n",
    "    patience=20,\n",
    "    batch_size=50,\n",
    "    eval_batch_size=100, #00,\n",
    "    scheduler_type = torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    cost_type = torch.nn.MSELoss\n",
    ")\n",
    "\n",
    "#Bundle of all learning-related parameters\n",
    "DEFAULT_LEARNING_CONFIG = dict(\n",
    "    net_config = DEFAULT_NET_CONFIG,\n",
    "    ensemble_config = DEFAULT_ENSEMBLE_CONFIG,\n",
    "    training_config = DEFAULT_TRAINING_CONFIG,\n",
    ")\n",
    "\n",
    "def assemble_dataset(raw_dataset):\n",
    "\n",
    "    features = torch.as_tensor(raw_dataset[:,SOLVER_INDEXES[\"input_slice\"]])\n",
    "#    print(\"Feature shape:\",features.shape)\n",
    "#    print(features.std(axis=0))\n",
    "    targets = torch.as_tensor(raw_dataset[:,SOLVER_INDEXES[\"output_slice\"]])\n",
    "#    print(\"Targets shape:\",targets.shape)\n",
    "#    print(targets.std(axis=0))\n",
    "    return torch.utils.data.TensorDataset(features,targets)\n",
    "\n",
    "#prototype, only covers ensemble uncertainties\n",
    "def retrain(db_path,learning_config=DEFAULT_LEARNING_CONFIG):\n",
    "\n",
    "    raw_dataset = getAllGNDData(db_path)\n",
    "    full_dataset = assemble_dataset(raw_dataset)\n",
    "\n",
    "\n",
    "    ensemble_config = learning_config[\"ensemble_config\"]\n",
    "    n_total = len(full_dataset)\n",
    "    n_test = int(ensemble_config[\"test_fraction\"]*n_total)\n",
    "    n_test = max(n_test,2)\n",
    "    n_train = n_total-n_test\n",
    "    print(\"Total / train / test points:\",n_total,n_train,n_test)\n",
    "\n",
    "    networks = []\n",
    "    network_errors = []\n",
    "    network_scores = []\n",
    "\n",
    "    successful_models = 0\n",
    "    i=0\n",
    "    while successful_models < ensemble_config[\"n_members\"]:\n",
    "        print(\"Training model {}\".format(i))\n",
    "        i += 1\n",
    "        if i > ensemble_config[\"max_model_tries\"]:\n",
    "            break\n",
    "        print(\"Good models found:\",successful_models)\n",
    "        print(\"Training ensemble member\",i)\n",
    "\n",
    "        train_data, test_data = torch.utils.data.random_split(full_dataset, (n_train, n_test))\n",
    "#        print(len(train_data), len(test_data ))\n",
    "\n",
    "\n",
    "        # This is a place where we could trivially parallelize training.\n",
    "\n",
    "        this_model = train_single_model(train_data, learning_config=learning_config)\n",
    "        model_score,model_errors = get_error_info(this_model,test_data)\n",
    "        print(\"Score:\",model_score)\n",
    "        if any(m < ensemble_config[\"score_thresh\"] for m in model_score):\n",
    "            print(\"Rejected.\")\n",
    "            continue\n",
    "        print(\"Accepted.\")\n",
    "\n",
    "        successful_models+=1\n",
    "\n",
    "        networks.append(this_model)\n",
    "        network_scores.append(model_score)\n",
    "        network_errors.append(model_errors)\n",
    "\n",
    "    error_info = np.mean(np.asarray(network_errors),axis=0)\n",
    "#    full_model = SOLVER_INDEXES( networks, error_info)\n",
    "    full_model = LearnerModel( networks, error_info)\n",
    "    full_model.calibrate(full_dataset)\n",
    "\n",
    "    return full_model\n",
    "\n",
    "\n",
    "def train_single_model(train_data, learning_config):\n",
    "\n",
    "    net_config = learning_config[\"net_config\"]\n",
    "    training_config = learning_config[\"training_config\"]\n",
    "\n",
    "    #Type parameters\n",
    "    activation_type = net_config[\"activation_type\"]\n",
    "    layer_type = net_config[\"layer_type\"]\n",
    "\n",
    "    #Splitting\n",
    "    n_total = len(train_data)\n",
    "    n_valid = int(training_config[\"validation_fraction\"]*n_total)\n",
    "    n_valid = max(n_valid,2)\n",
    "    n_train = n_total-n_valid\n",
    "    #print(type(train_data),type(train_data[:]))\n",
    "\n",
    "    #Stupid fix for now, but this trick doesn't hurt anything and enables the #Normalizing line to work\n",
    "    train_data.indices = np.asarray(train_data.indices)\n",
    "    train_data, valid_data = torch.utils.data.random_split(train_data, (n_train, n_valid))\n",
    "    #print(type(train_data),type(train_data[:]))\n",
    "\n",
    "    #Normalizing\n",
    "    train_features, train_labels = (train_data[:])\n",
    "\n",
    "    inscaler = Scaler.from_tensor(train_features)\n",
    "    cost_scaler = Scaler.from_tensor(train_labels)\n",
    "    outscaler = Scaler.from_inversion((cost_scaler))\n",
    "\n",
    "    #Size parameters\n",
    "    indices = SOLVER_INDEXES\n",
    "\n",
    "    n_inputs = indices[\"n_inputs\"]\n",
    "    n_outputs = indices[\"n_outputs\"]\n",
    "    n_hidden = net_config[\"n_hidden\"]\n",
    "\n",
    "    layers = [inscaler,layer_type(n_inputs,n_hidden),activation_type()]\n",
    "\n",
    "    for i in range(net_config[\"n_layers\"]-2):\n",
    "        layers.append(layer_type(n_hidden,n_hidden))\n",
    "        layers.append(activation_type())\n",
    "    layers.append(layer_type(n_hidden,n_outputs))\n",
    "\n",
    "    train_network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    cost_fn = training_config[\"cost_type\"]()\n",
    "    opt = training_config[\"optimizer_type\"](train_network.parameters(),lr=training_config[\"lr\"])\n",
    "    patience = training_config[\"patience\"]\n",
    "    scheduler = training_config[\"scheduler_type\"](opt,patience=patience,verbose=False,factor=0.5)\n",
    "\n",
    "\n",
    "    best_cost = np.inf\n",
    "    best_params = train_network.state_dict()\n",
    "\n",
    "    train_dloader = torch.utils.data.DataLoader(train_data,batch_size=training_config[\"batch_size\"],shuffle=True)\n",
    "    valid_dloader = torch.utils.data.DataLoader(valid_data,batch_size=training_config[\"eval_batch_size\"],shuffle=False)\n",
    "\n",
    "    # Need to add scales for costs\n",
    "    for i in range(training_config[\"n_epochs\"]):\n",
    "        train_epoch(train_dloader,train_network,cost_fn,opt,scaler=cost_scaler)\n",
    "        eval_cost = evaluate_dataset_errors(valid_dloader,train_network,scaler=cost_scaler).abs().mean().item()\n",
    "        #if i%100==0: print(eval_cost)\n",
    "        scheduler.step(eval_cost)\n",
    "        if eval_cost < best_cost:\n",
    "            #print(\"best epoch:\",i)\n",
    "            best_cost = eval_cost\n",
    "            best_params = copy.deepcopy(train_network.state_dict())\n",
    "            boredom = 0\n",
    "        else:\n",
    "            boredom +=1\n",
    "\n",
    "        if boredom > 2*patience+1:\n",
    "            print(\"Training finalized at epoch\",i)\n",
    "            break\n",
    "    else:\n",
    "        print(\"Training finished due to max epoch\",i)\n",
    "\n",
    "\n",
    "    train_network.load_state_dict(best_params)\n",
    "    real_scale_network = torch.nn.Sequential(*train_network[:], outscaler)\n",
    "\n",
    "    for param in real_scale_network.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    return real_scale_network\n",
    "\n",
    "\n",
    "def train_epoch(train_data,network,cost_fn,opt,scaler):\n",
    "    for batch_in,batch_out in train_data:\n",
    "        opt.zero_grad()\n",
    "        batch_pred = network(batch_in)\n",
    "        cost = cost_fn(batch_pred,scaler(batch_out))\n",
    "        cost.backward()\n",
    "        opt.step()\n",
    "\n",
    "def evaluate_dataset_errors(dloader,network,scaler,show=False):\n",
    "    predicted = []\n",
    "    true = []\n",
    "    with torch.autograd.no_grad():\n",
    "        for bin,bout in dloader:\n",
    "\n",
    "            bpred = network(bin)\n",
    "            if scaler is not None:\n",
    "                bout = scaler(bout)\n",
    "\n",
    "            true.append(bout)\n",
    "            predicted.append(bpred)\n",
    "\n",
    "    true = torch.cat(true,dim=0)\n",
    "    predicted = torch.cat(predicted,dim=0)\n",
    "    err = predicted-true\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "\n",
    "def build_network(net_config):\n",
    "    pass\n",
    "    #do we need this? maybe for serializing more efficiently?\n",
    "\n",
    "def l1_score(true,predicted):\n",
    "    sum_abs_resid = np.abs(true-predicted).sum()\n",
    "    med = np.median(true)\n",
    "    sum_abs_dev = np.abs(true-med).sum()\n",
    "    return 1 - sum_abs_resid/sum_abs_dev\n",
    "\n",
    "\n",
    "def get_error_info(network,test_dset):\n",
    "    test_dloader = torch.utils.data.DataLoader(test_dset,batch_size=100,shuffle=False)\n",
    "\n",
    "    error = evaluate_dataset_errors(test_dloader,network,scaler=None).numpy()\n",
    "    true = test_dset[:][-1].numpy()\n",
    "    predicted = true + error\n",
    "    return get_score(predicted,true)\n",
    "\n",
    "def get_score(predicted,true):\n",
    "    score = []\n",
    "    rmse_list = []\n",
    "    for i,(p,t) in enumerate(zip(predicted.T,true.T)):\n",
    "\n",
    "        rmse = np.sqrt(sklearn.metrics.mean_squared_error(t,p))\n",
    "\n",
    "        if p.std() < 1e-300 and t.std() < 1e-300:\n",
    "            rsq = 1.\n",
    "            l1resid = 1.\n",
    "        else:\n",
    "            rsq = sklearn.metrics.r2_score(t,p)\n",
    "            l1resid = l1_score(t,p)\n",
    "\n",
    "        score.append(rsq)\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "    #HARDCODED: TOTAL SCORE IS PRODUCT OF SCORES FOR EACH TARGET\n",
    "    score = np.asarray(score)\n",
    "    #score = np.prod(score*(score>0))\n",
    "    rmse_list = np.asarray(rmse_list)\n",
    "    return score,rmse_list\n",
    "\n",
    "class Scaler(torch.nn.Module):\n",
    "    def __init__(self,means,stds,eps=1e-300):\n",
    "        super().__init__()\n",
    "        self.means = torch.nn.Parameter(means,requires_grad=False)\n",
    "        self.stds = torch.nn.Parameter(stds,requires_grad=False)\n",
    "        self.eps  = eps\n",
    "\n",
    "    @classmethod\n",
    "    def from_tensor(cls,tensor):\n",
    "        means = tensor.mean(dim=0)\n",
    "        stds = tensor.std(dim=0)\n",
    "        return cls(means,stds)\n",
    "\n",
    "    @classmethod\n",
    "    def from_inversion(cls,other):\n",
    "        new_stds = 1/(other.stds+other.eps)\n",
    "        new_means = -other.means/(other.stds+other.eps)\n",
    "        return cls(new_means,new_stds)\n",
    "\n",
    "    def forward(self,tensor):\n",
    "        return (tensor-self.means)/(self.stds + self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4d3208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total / train / test points: 30410 27369 3041\n",
      "Training model 0\n",
      "Good models found: 0\n",
      "Training ensemble member 1\n",
      "Training finished due to max epoch 199\n",
      "Score: [0.9999021  0.99979326 0.99984464 0.99734026 0.99901023 0.99984421\n",
      " 0.99735123 0.99901036 0.99984418]\n",
      "Accepted.\n",
      "Training model 1\n",
      "Good models found: 1\n",
      "Training ensemble member 2\n",
      "Training finished due to max epoch 199\n",
      "Score: [0.99989664 0.99978431 0.9998346  0.99720405 0.99916727 0.99984173\n",
      " 0.99720525 0.9991668  0.99984175]\n",
      "Accepted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/f0qzb85j0p102grvg_s4spsm5jr2zx/T/ipykernel_99101/1905036015.py:106: UserWarning: Inactive columns detected: (array([], dtype=int64),) , they will not be included in isokay.\n",
      "  warnings.warn(\"Inactive columns detected: {} , they will not be included in isokay.\".format(np.where(inactive_columns)))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import nn_learner\n",
    "\n",
    "DB_PATH='SOLEDGE.db'\n",
    "\n",
    "model =retrain(db_path=DB_PATH)\n",
    "\n",
    "#Ensure model can be pickled and unpickled.\n",
    "torch.save(model,\"data.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0572d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "Inputs = collections.namedtuple('SOLEDGE', 'R Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e598c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a2db93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outputs(BPHI=0.0008147176460437944, BR=0.00042856495610263043, BZ=0.0002587362656812399, VE=1.391538751567716e+21, NE=2.5059892907314106e+17, TE=0.20415088695009453, VI=1.1775234058097163e+21, NI=2.4982174392239597e+17, TI=0.2058051179257796)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir = 2.8\n",
    "iz= -0.5\n",
    "\n",
    "model(Inputs(ir, iz))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decbd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
